# Import Quix Streams and other necessary libraries
import os
import logging
import joblib
import pandas as pd
from dotenv import load_dotenv
import json
from quixstreams import Application
from influxdb_client import InfluxDBClient, Point, WriteOptions
from sklearn.ensemble import RandomForestClassifier

# For local development, load environment variables from a .env file
load_dotenv()

# --- Kafka and Log Configuration ---
log_level = getattr(logging, os.getenv("LOG_LEVEL", "INFO").upper(), logging.INFO)
logging.basicConfig(level=log_level, format='[%(asctime)s] [%(levelname)s] %(message)s', datefmt='%Y-%m-%d %H:%M:%S')

KAFKA_BROKER = os.getenv("KAFKA_BROKER", "172.16.2.117:9092")
KAFKA_INPUT_TOPIC = os.getenv("KAFKA_INPUT_TOPIC", "event-frames-model")
KAFKA_ML_TOPIC = os.getenv("KAFKA_ML_TOPIC", "taxi-demand-anomalies")
CONSUMER_GROUP = os.getenv("CONSUMER_GROUP", "taxi-anomaly-detector")

INFLUX_URL = "http://172.16.2.117:8086"  # os.getenv("INFLUX_URL")
INFLUX_TOKEN = os.getenv("INFLUX_TOKEN")
INFLUX_ORG = os.getenv("INFLUX_ORG")
INFLUX_BUCKET = os.getenv("INFLUX_BUCKET")

print("kafka broker:", KAFKA_BROKER)

# Initialize InfluxDB client
try:
    influx_client = InfluxDBClient(url=INFLUX_URL, token=INFLUX_TOKEN, org=INFLUX_ORG)
    influx_writer = influx_client.write_api(write_options=WriteOptions(batch_size=1, flush_interval=1))
    logging.info("‚úÖ InfluxDB client initialized successfully")
except Exception as e:
    logging.error(f"‚ùå Failed to initialize InfluxDB client: {e}")
    exit(1)

if not all([KAFKA_BROKER, KAFKA_INPUT_TOPIC, KAFKA_ML_TOPIC]):
    raise ValueError("Missing required environment variables for Kafka.")

# --- Resolve Model Path (ENV ‚Üí fixed D:\ ‚Üí uploaded ‚Üí local folder) ---
def resolve_model_path() -> str:
    # 1) ENV (if provided)
    env_path = os.getenv("MODEL_PATH")
    if env_path and os.path.exists(env_path):
        return env_path

    # 2) Fixed Windows path requested by user
    win_path = r"D:\WORK\Project_Year4\miniproject\Iot-class-2025-online-ml-predict\app\rf_fd001.pkl"
    if os.path.exists(win_path):
        return win_path

    # 3) Uploaded file in this environment (if running here)
    uploaded_path = "/mnt/data/rf_fd001.pkl"
    if os.path.exists(uploaded_path):
        return uploaded_path

    # 4) Fallback: same folder as script
    script_dir = os.path.dirname(os.path.realpath(__file__))
    local_path = os.path.join(script_dir, "rf_fd001.pkl")
    if os.path.exists(local_path):
        return local_path

    # If nothing is found, return the Windows path (so the error message is clear for your machine)
    return win_path

# --- Load the Model ---
try:
    model_path = resolve_model_path()
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"Model file not found at: {model_path}")
    rf_model: RandomForestClassifier = joblib.load(model_path)
    logging.info(f"‚úÖ RandomForest model loaded successfully from: {model_path}")
except Exception as e:
    logging.error(f"‚ùå Failed to load the model: {e}")
    raise

# --- Initialize Quix Application ---
app = Application(
    broker_address=KAFKA_BROKER,
    consumer_group=CONSUMER_GROUP,
    loglevel="INFO",
    state_dir=os.path.dirname(os.path.abspath(__file__)) + "/state/",
    auto_offset_reset="earliest"
)

# Define input and output topics
input_topic = app.topic(KAFKA_INPUT_TOPIC, value_deserializer="json")
# ‡πÉ‡∏ä‡πâ JSON serializer ‡∏Ç‡∏≠‡∏á Quix ‡πÇ‡∏î‡∏¢‡∏™‡πà‡∏á dict ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢ (‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á encode ‡πÄ‡∏≠‡∏á)
output_topic = app.topic(KAFKA_ML_TOPIC)

producer = app.get_producer()

# A simple buffer to store historical data for 'Lag' and 'Rolling_Mean'
data_buffer = []
BUFFER_SIZE = 20  # ‡∏´‡∏±‡πà‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• 20 ‡∏à‡∏∏‡∏î (‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î‡∏ô‡∏µ‡πâ)

def handle_message(row_data: dict):
    try:
        sensor_name = row_data.get("name", "")
        payload = row_data.get("payload", {}) or {}

        # Features ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏ï‡∏≠‡∏ô train (‡∏ï‡∏≤‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ó‡∏µ‡πà‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ)
        feature_order = ["T24","T30","T50","P15","P30","Nf","Nc","Ps30","phi","NRf","NRc","BPR","htBleed","W31","W32"]


        # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° features (‡∏Å‡∏±‡∏ô key ‡∏´‡∏≤‡∏¢ ‚Üí ‡πÄ‡∏ï‡∏¥‡∏° 0.0)
        try:
            feats = [[float(payload.get(f, 0.0)) for f in feature_order]]
        except Exception as e:
            logging.error(f"‚ùå Feature extraction error: {e}, payload keys={list(payload.keys())}")
            return

        # Predict (RandomForestClassifier)
        try:
            prediction = int(rf_model.predict(feats)[0])
        except Exception as e:
            logging.error(f"‚ùå Model predict() failed: {e}")
            return

        prob = None
        if hasattr(rf_model, "predict_proba"):
            try:
                prob = float(rf_model.predict_proba(feats)[0, 1])
            except Exception as e:
                logging.warning(f"‚ö†Ô∏è predict_proba failed: {e}")
                prob = None

        # Timestamp
        ts = int(payload.get("timestamp", pd.Timestamp.utcnow().timestamp()*1000))
        ts_val = pd.to_datetime(ts, unit="ms", utc=True)

        # Build output payload
        result_payload = {
            "RUL_predicted": prediction,  # ‡∏´‡∏£‡∏∑‡∏≠‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠ field ‡∏≠‡∏∑‡πà‡∏ô ‡πÄ‡∏ä‡πà‡∏ô 'class_predicted'
            "prob_warning": prob,
            "timestamp": ts,
            "date": ts_val.isoformat()
        }

        result = {
            "id": row_data.get("id", ""),
            "name": sensor_name,
            "place_id": row_data.get("place_id", ""),
            "payload": result_payload
        }

        # ‡∏™‡πà‡∏á‡πÑ‡∏õ Kafka (‡πÉ‡∏ä‡πâ value_serializer="json" ‡πÅ‡∏•‡πâ‡∏ß ‡∏à‡∏∂‡∏á‡∏™‡πà‡∏á dict ‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢)
        serialized_data = json.dumps(result).encode("utf-8")
        producer.produce(
            topic=output_topic.name,
            value=serialized_data,
            timestamp=ts
        )
        logging.info(f"‚úÖ Published to {KAFKA_ML_TOPIC} - data: {result}")

        # ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô InfluxDB (‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: ‡πÄ‡∏Å‡πá‡∏ö‡∏Ñ‡πà‡∏≤ prediction ‡πÄ‡∏õ‡πá‡∏ô field)
        try:
            point = (
                Point(KAFKA_ML_TOPIC)
                .tag("name", sensor_name)
                .field("RUL_predicted", prediction)
                .time(ts_val)
            )
            influx_writer.write(bucket=INFLUX_BUCKET, org=INFLUX_ORG, record=point)
            logging.info("[üìä] Wrote prediction to InfluxDB")
        except Exception as e:
            logging.error(f"‚ùå Failed to write to InfluxDB: {e}")

        return result

    except Exception as e:
        logging.error(f"‚ùå Error processing message: {e}")

# Run the application
if __name__ == "__main__":
    sdf = app.dataframe(input_topic)
    sdf = sdf.apply(handle_message)
    app.run(sdf)
